---
title: "HW2"
output: github_document
date: "2024-09-26"
---

```{r setup, include = FALSE}

library(tidyverse) #read csv
library(readxl) #read excel
library(haven) #read SAS
library(dplyr)
```

## Problem 1

```{r include = FALSE}
transit_df = 
  read.csv("Data_Imports/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c(".", "NA", "")) %>% 
  janitor::clean_names() %>% 
  select(line:entry, vending, ada) %>% 
  mutate(
    entry = case_match(
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE
  )
) 
```
After cleaning the initial NYC transit dataset, the resulting dataset is described by **`r ncol(transit_df)`** variables, which are: **`r colnames(transit_df)`**. This dataset is **`r nrow(transit_df)`** rows by **`r ncol(transit_df)`** columns. The initial cleaning steps included changing column names to make sure they were all formatted the same way (in snake_case), removing certain columns not necessary for data analysis, and converting the data points under the entry column into logical variables. Because the routes are all separated into different columns, this data isn't tidy. To tidy, the data set would contain one column that held the information for route number and another column that held the information on whether there was a train that followed that route at each location.

```{r include = FALSE}
line_station_df = 
  transit_df %>% 
  mutate(line_station = paste(line, station_name, sep= " ")) %>% 
  distinct(line_station, .keep_all = TRUE)
```

```{r include = FALSE}
ada_compliant_df = 
  line_station_df %>% 
  filter(ada == TRUE)
```

```{r include = FALSE}
no_vending_df = 
  line_station_df %>% 
  filter(vending == "NO")

entrance_no_vending_df =
  no_vending_df %>% 
  filter(entry == TRUE)

prop_entrance_no_vending = nrow(entrance_no_vending_df) / nrow(no_vending_df)
```

```{r include = FALSE}
transit_tidy_A_df = 
  transit_df %>% 
  mutate(across(c(route1:route11), as.character)) %>% 
  pivot_longer(
    route1:route11,
    names_to = "routes", 
    values_to = "trains") %>% 
  mutate(line_station = paste(line, station_name, sep= " ")) %>% 
  distinct(line_station, .keep_all = TRUE) %>% 
  filter(trains == "A")

transit_tidy_ada_df = 
  transit_tidy_A_df %>% 
  filter(ada == TRUE)
```

There are **`r nrow(line_station_df)`** distinct stations.

There are **`r nrow(ada_compliant_df)`**  ADA compliant stations.

The proportion of stations with no vending that allow entry is **`r prop_entrance_no_vending`**

There are **`r nrow(transit_tidy_A_df)`** distinct stations that serve the A train. Of these stations, **`r nrow(transit_tidy_ada_df)`** are ADA compliant

## Problem 2

```{r include = FALSE}
mr_trash_df = 
  read_excel("Data_Imports/202309_Trash_Wheel_Collection_Data.xlsx", 
             sheet = "Mr. Trash Wheel",
             range = "A2:N586",
             na = c(".", "NA", ""),
             ) %>% 
  janitor::clean_names() %>% 
  mutate(
    vessel_name = "Mr. Trash Wheel",
    sports_balls = as.integer(round(sports_balls)),
    year = as.double(year)
    ) %>% 
  relocate(vessel_name)
```

```{r include = FALSE}
prof_trash_df = 
  read_excel("Data_Imports/202309_Trash_Wheel_Collection_Data.xlsx", 
             sheet = "Professor Trash Wheel",
             range = "A2:M108",
             na = c(".", "NA", ""),
             ) %>% 
  janitor::clean_names()%>% 
  mutate(
    vessel_name = "Professor Trash Wheel",
    sports_balls = "",
    sports_balls = as.integer(sports_balls)
    )%>% 
  relocate(vessel_name, 1:12, sports_balls)
```

```{r include = FALSE}
gwynnda_trash_df = 
  read_excel("Data_Imports/202309_Trash_Wheel_Collection_Data.xlsx", 
             sheet = "Gwynnda Trash Wheel",
             range = "A2:L157",
             na = c(".", "NA", ""),
             ) %>% 
  janitor::clean_names()%>% 
  mutate(
    vessel_name = "Gwynnda Trash Wheel",
    sports_balls = "",
    glass_bottles = "",
    sports_balls = as.integer(sports_balls),
    glass_bottles = as.double(glass_bottles)
    )%>% 
  relocate(vessel_name, 1:9, glass_bottles, 10:11, sports_balls)

```

```{r include = FALSE}
combined_trash_df = bind_rows(mr_trash_df, prof_trash_df, gwynnda_trash_df)

gwynnda_june22_df =
  gwynnda_trash_df %>% 
  filter(year == 2022) %>% 
  filter(month == "June")
```

Problem two combine three data set: data from Mr. Trash Wheel, Professor Trash Wheel, and Gwynda Trash Wheel. The combined data set is **`r nrow(combined_trash_df)`** rows by **`r ncol(combined_trash_df)`** columns. The data is separated by the amount of Plastic Bottle, Polystyrene, Cigarette Butts, Glass Bottles, Plastic Bags, Wrappers, and Sports Balls each vessel cleaned up. The total trash Professor Trash Wheel cleaned up was **`r sum(pull(prof_trash_df, weight_tons))`** tons. The amount of cigaretter butts Gwynnda picked up in June 2022 was **`r sum(pull(gwynnda_june22_df, cigarette_butts))`** butts.


## Problem 3
```{r include = FALSE}
bakers_df = 
  read.csv("Data_Imports/gbb_datasets/bakers.csv", na = c(".", "NA", "", "N/A")) %>% 
  janitor::clean_names() %>% 
  arrange(series) %>% 
  separate(baker_name, into = c("baker_first_name", "baker_last_name"), sep = " ")
```
  
```{r include = FALSE}
bakes_df = 
  read.csv("Data_Imports/gbb_datasets/bakes.csv", na = c(".", "NA", "", "N/A")) %>% 
  janitor::clean_names() %>% 
  mutate(baker = ifelse(baker == '"Jo"', "Jo", baker)) %>% 
  rename(baker_first_name = baker) 
```  

```{r include = FALSE}
results_df = 
  read.csv("Data_Imports/gbb_datasets/results.csv", na = c(".", "NA", "", "N/A"), 
          skip = 3,
          header = FALSE, 
          col.names = c("series", "episode", "baker_first_name", "technical", "result")) %>% 
  mutate(baker = ifelse(baker_first_name == "Joanne", "Jo", baker_first_name))
```

```{r include = FALSE}
gbb_half_df = left_join(results_df, bakes_df, 
                        by = c("baker_first_name"="baker_first_name", "series"="series", "episode"="episode"))
gbb_full_df = left_join(gbb_half_df, bakers_df, 
                        by = c("baker_first_name"="baker_first_name", "series"="series")) %>% 
  relocate(series, baker_first_name, 8:11, episode, 4:7)

write.csv(gbb_full_df, "Data_Imports/gbb_datasets/gbb_full_dataset.csv" )
```

  The first part of problem 3 looks at the three .csv files to combine them into 1. The first, **bakers.csv**, describes the name, age, and occupation of each contestant from season 1 to season 10. To organize this data set, the corresponding data frame in r arranged the rows in ascending order by season to match the other data sets and also split the "name" column into a first name column and last name column. This was done to make it easier to merge the data sets. The second .csv file, **bakes.csv**, looks at each contestant's show stopper and signature bakes from every episode in seasons 1 to 8. To clean this data, the name "Jo"  was changed to Jo to match the values in the other data sets. The column name for baker was also changed to baker first name to match the baker data set. The last .csv file was the **results.csv**, which gave every contestant's technical score and their result (in/out/star baker for most episodes, runner-up/winner for the season finale) for each episode in seasons 1 to 10. To clean this data, the first 3 rows were skipped as they contained no data, and the columns were renamed as they were given no headers in the original data set. Joanne was renamed to Jo to match the other two data sets.

The three data sets were merged after the cleaning steps, and the columns were re-arranged to make it easier to follow along all the data. The full data set first gives the series and then baker information, followed by their technical scores, results, and bakes from each episode. The main question from this process for me came from how to organize the data- namely, I split the series and episode number since I thought it would be easier to follow the scores/results from each episode if those three columns were next to each other, but I know usually episode and series number are kept together.

Below is ome of the data from the full data set. Specifically, the data shows all the star bakers from seasons 5 to 10:


```{r echo = FALSE}
gbb_table_df = 
  gbb_full_df %>% 
  filter(series %in% (5:10)) %>% 
  filter(result == "STAR BAKER")

knitr::kable(gbb_table_df, format = "html" )
```

For season 5, Richard received the most star bakes but was runner-up to Nancy, who only got star baker once in this season. In season 6, Ian and Nadiya both were star bakers 3 times, which corresponded them getting runner up and winner for this season, respectively. This followed similar trends with Candice (winner) and Andrew (runner up) in season 7; with Steven (runner up) and Sophie in season 8; and Kim-Joy and Ruby (runners up) and Rahul in season 9 in terms of more star bakes equating to being runner-up and winner. In season 10, however, Steph had the most star bakes but she was runner-up to David, who had no star bakes.


```{r include = FALSE}
viewers_df = 
  read.csv("Data_Imports/gbb_datasets/viewers.csv", na = c(".", "NA", "", "N/A")) %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    series_1:series_10,
    names_to = "series", 
    values_to = "views")
```

```{r include = FALSE}
viewers_s1_df = 
  viewers_df %>% 
  filter (series == "series_1")
  
viewers_s5_df = 
  viewers_df %>% 
  filter (series == "series_5")

viewers_table_df = 
  viewers_df %>% 
  head(10)
```

This last table shows the first 10 rows of the tidied viewers data set, which reports the amount of viewers each episode in seasons 1 to 10 garnered: 

```{r echo = FALSE}
knitr::kable(viewers_table_df, format = "html" )
```

The average viewers (in millions of people) for season 1 was **`r mean(pull(viewers_s1_df, views), na.rm = TRUE)`**, while it was **`r mean(pull(viewers_s5_df, views), na.rm = TRUE)`** in season 5
